---
slideinit: "<section markdown=\"1\" data-background=\"https://yyyu200.github.io/DFTbook/img/slidebackground.png\"><section markdown=\"1\">"
vertical: "</section><section markdown=\"1\">"
horizontal: "</section></section><section markdown=\"1\" data-background=\"https://yyyu200.github.io/DFTbook/img/slidebackground.png\"><section markdown=\"1\">"
layout: post
title: 反向传播算法
author: yyyu200
tags: Deep Learning
subtitle: 
category: Blogs
notebookfilename: intro
visualworkflow: true
published: true
theme: beige
trans: cube
---

# 反向传播算法

 在前向算法中，需要w参与运算，w是网络中各个连接上的权重，这些值需要在训练过程中确定。在多层感知机中获取隐含层的权重是很困难的，我们可以计算输出层的误差更新参数，但是隐含层误差则难以直接定义。反向传播的思想是，预测值与真实值的差别可以评估输出层的误差，计算在最后一个隐含层中的每个神经元对输出层误差影响，最后一层隐含层的误差又由其前一层的隐含层的计算得出。如此类推，直到输入层(1)。

$x_j=\sum_iy_iw_{ji}$

$y_j=\frac{1}{1+e^{-x_i}}$

$E=\frac{1}{2}\sum_c\sum_j(y_{j,c}-d_{j,c})^2$

$\frac{\partial E}{\partial y_j}=y_j-d_j$

$\frac{\partial E}{\partial x_j}=\frac{\partial E}{\partial y_j}y_j(1-y_j)$

$\frac{\partial E}{\partial w_{ji}}=\frac{\partial E}{\partial x_j}·\frac{\partial x_j}{\partial w_{ji}}=\frac{\partial E}{\partial x_j}y_i$

$\frac{\partial E}{\partial y_i}=\sum_j\frac{\partial E}{\partial x_j}·w_{ji}$

$\Delta w=-\varepsilon\frac{\partial E}{\partial w}$

$\Delta w(t)=-\varepsilon\frac{\partial E}{\partial w(t)}+\alpha\Delta w(t-1)$


# Reference
1. Learning representations by back-propagating errors, David E. Rumelhart, Geoffrey E. Hinton & Ronald J. Williams, Nature volume 323, pages533–536 (1986) 

2. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/; http://www.cnblogs.com/charlotte77/p/5629865.html

3. https://blog.csdn.net/holmosaint/article/details/82193134
